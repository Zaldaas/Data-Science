# -*- coding: utf-8 -*-
"""Zeid Aldaas Homework 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1na1RZ95OkGzP1e2BpD0DcqOnjjrpKB7z
"""

import pandas as pd

# 1. Consider the toy dataset below which shows if 4 subjects have diabetes or not, along with two diagnostic measurements. (Note: do NOT write any code for this problem. The answers are to be computed by hand.)

# a. The "Class" variable is HasDiabetes, which indicates whether a subject has diabetes (Yes or No).
# b. Normalize the Preg and Glucose values by scaling the minimum-maximum range of each column to 0-1. Fill in the empty columns in the table.
'''
1. Preg.Norm
  Row 1: (2 - 1) / (3 - 1) = 0.5
  Row 2: (3 - 1) / (3 - 1) = 1.0
  Row 3: (2 - 1) / (3 - 1) = 0.5
  Row 4: (1 - 1) / (3 - 1) = 0.0
  Row 5: (2 - 1) / (3 - 1) = 0.5

2. Glucose.Norm
  Row 1: (157 - 77) / (174 - 77) = 0.8247
  Row 2: (174 - 77) / (174 - 77) = 1.0
  Row 3: (105 - 77) / (174 - 77) = 0.2887
  Row 4: (77 - 77) / (174 - 77) = 0.0
  Row 5: (94 - 77) / (174 - 77) = 0.1752
'''
# c. Predict whether a subject with Preg=2, Glucose=94 will have diabetes using the 1-NN algorithm and
# i. Using Euclidean distance on the original variables
'''
distance = ‚àö((Preg - 2)^2 + (Glucose - 94)^2)

Row 1: ‚àö((2 - 2)^2 + (157 - 94)^2) = ‚àö(0 + 63^2) = 63.0
Row 2: ‚àö((3 - 2)^2 + (174 - 94)^2) = ‚àö(1 + 80^2) ‚âà 80.0063
Row 3: ‚àö((2 - 2)^2 + (105 - 94)^2) = ‚àö(0 + 11^2) = 11.0
Row 4: ‚àö((1 - 2)^2 + (77 - 94)^2) = ‚àö(1 + 17^2) ‚âà 17.0294

Nearest Neighbor: Row 3
Prediction: Yes (HasDiabetes = Yes for Row 3)
'''
# ii. Using Euclidean distance on the normalized variables
'''
Target (Preg.Norm = 0.5, Glucose.Norm = 0.1752)

Row 1: ‚àö((0.5 - 0.5)^2 + (0.8247 - 0.1752)^2) ‚âà 0.6495
Row 2: ‚àö((1.0 - 0.5)^2 + (1.0 - 0.1752)^2) ‚âà 0.9351
Row 3: ‚àö((0.5 - 0.5)^2 + (0.2887 - 0.1752)^2) ‚âà 0.1135
Row 4: ‚àö((0.0 - 0.5)^2 + (0.0 - 0.1752)^2) ‚âà 0.5293

Nearest Neighbor: Row 3
Prediction: Yes (HasDiabetes = Yes for Row 3)
'''

# 2. The pima-indians-diabetes-resampled.csv file on Canvas contains records indicating whether the subjects have diabetes or not, along with certain diagnostic measurements. All subjects are of Pima Indian heritage and this dataset is called the Pima Indian Diabetes Database. The goal is to see if it is possible to predict if a subject has diabetes given some of the diagnostic measurements. (Note: this problem is an extension of the classwork assignment; Python code from the class is also posted on Canvas.)
# a. Read the data file
data = pd.read_csv('pima-indians-diabetes-resampled.csv')
# b. What does ‚ÄúPreg‚Äù represent in the dataset? (2-3 sentences. Search for the Pima Indian Diabetes Database online. Its background and the ethics issues it raises are also important.
# ANSWER: "Preg" represents the number of times a subject has been pregnant. In the context of the Pima Indian Diabetes Database, it serves as one of the diagnostic features used to predict diabetes. This dataset, collected from a specific population, raises ethical questions about the representation and health privacy of Indigenous communities.
# c. 0 values in the Glucose column indicate missing values. Remove rows which contain missing values in the Glucose column. You should have 763 rows. [code]
data['Glucose'].replace(0, pd.NA, inplace=True)
data = data.dropna(subset=['Glucose'])
print(data)

# d. Create three new columns/variables which are the normalized versions of Preg, Pedigree, and Glucose columns, scaling the minimum-maximum range of each column to 0-1 (you can use the code developed in class). [code]
data['Preg.Norm'] = (data['Preg'] - data['Preg'].min()) / (data['Preg'].max() - data['Preg'].min())
data['Pedigree.Norm'] = (data['Pedigree'] - data['Pedigree'].min()) / (data['Pedigree'].max() - data['Pedigree'].min())
data['Glucose.Norm'] = (data['Glucose'] - data['Glucose'].min()) / (data['Glucose'].max() - data['Glucose'].min())
print(data)

# e. Split the dataset into train and test datasets with the first 500 rows for training, and the remaining rows for test. Do NOT randomly sample the data (though resampling is usually done, this hw problem does not use this step for ease of grading).
train_data = data.iloc[:500]
test_data = data.iloc[500:]
print(train_data)
print(test_data)

# f. Train and test a k-nearest neighbor classifier with the dataset. Consider only the normalized Preg and Pedigree columns. Set k=1. What is the error rate (number of misclassifications)? [code, error rate]
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score


knn_1 = KNeighborsClassifier(n_neighbors=1)
knn_1.fit(train_data[['Preg.Norm', 'Pedigree.Norm']], train_data['HasDiabetes'])


predictions_1 = knn_1.predict(test_data[['Preg.Norm', 'Pedigree.Norm']])
error_rate_1 = 1 - accuracy_score(test_data['HasDiabetes'], predictions_1)

print(error_rate_1)

# g. Repeat part (f) but consider the normalized Preg, Pedigree, and Glucose columns. Set k=1. What is the error rate? Will the error rate always decrease with a larger number of features? Why or why not: answer in 2-3 sentences? [code, error rate, answer]

knn_1_with_glucose = KNeighborsClassifier(n_neighbors=1)
knn_1_with_glucose.fit(train_data[['Preg.Norm', 'Pedigree.Norm', 'Glucose.Norm']], train_data['HasDiabetes'])


predictions_1_with_glucose = knn_1_with_glucose.predict(test_data[['Preg.Norm', 'Pedigree.Norm', 'Glucose.Norm']])
error_rate_1_with_glucose = 1 - accuracy_score(test_data['HasDiabetes'], predictions_1_with_glucose)

print(error_rate_1_with_glucose)

# ANSWER: No, adding more features does not always reduce the error rate. Extra features can introduce noise and increase the model's complexity, potentially leading to overfitting. It's essential to select features that are truly informative.

# h. Repeat part (g) but set k=5. What is the error rate? [code, error rate]

knn_5 = KNeighborsClassifier(n_neighbors=5)
knn_5.fit(train_data[['Preg.Norm', 'Pedigree.Norm', 'Glucose.Norm']], train_data['HasDiabetes'])


predictions_5 = knn_5.predict(test_data[['Preg.Norm', 'Pedigree.Norm', 'Glucose.Norm']])
error_rate_5 = 1 - accuracy_score(test_data['HasDiabetes'], predictions_5)

print(error_rate_5)

# i. Repeat part (h) but set k=11. What is the error rate?  [code, error rate]

knn_11 = KNeighborsClassifier(n_neighbors=11)
knn_11.fit(train_data[['Preg.Norm', 'Pedigree.Norm', 'Glucose.Norm']], train_data['HasDiabetes'])


predictions_11 = knn_11.predict(test_data[['Preg.Norm', 'Pedigree.Norm', 'Glucose.Norm']])
error_rate_11 = 1 - accuracy_score(test_data['HasDiabetes'], predictions_11)

print(error_rate_11)

# j. Considering your observations from (g)-(i), which is the best value for k? [answer]
# ANSWER: The best value for k is the one with the lowest error rate, observed from the results of parts (g)-(i).

# Commented out IPython magic to ensure Python compatibility.
def colab2pdf():
  # @title Download Notebook in PDF Format{display-mode:'form'}
  !apt-get install -yqq --no-install-recommends librsvg2-bin>/dev/null;
  import contextlib,datetime,google,io,IPython,ipywidgets,json,locale,nbformat,os,pathlib,requests,urllib,warnings,werkzeug,yaml,re;locale.setlocale(locale.LC_ALL,'en_US.UTF-8');warnings.filterwarnings('ignore',category=nbformat.validator.MissingIDFieldWarning);
#   %matplotlib inline
  def convert(b):
    try:
      s.value='üîÑ Converting';b.disabled=True
      n=pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f'http://{os.environ["COLAB_JUPYTER_IP"]}:{os.environ["KMP_TARGET_PORT"]}/api/sessions').json()[0]['name'])))
      p=pathlib.Path('/content/pdfs')/f'{datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")}_{n.stem}';p.mkdir(parents=True,exist_ok=True);nb=nbformat.reads(json.dumps(google.colab._message.blocking_request('get_ipynb',timeout_sec=600)['ipynb']),as_version=4)
      u=[u for c in nb.cells if c.get('cell_type')=='markdown' for u in re.findall(r'!\[.*?\]\((https?://.*?)\)',c['source']) if requests.head(u,timeout=5).status_code!=200]
      if u:raise Exception(f"Bad Image URLs: {','.join(u)}")
      nb.cells=[cell for cell in nb.cells if '--Colab2PDF' not in cell.source]
      nb=nbformat.v4.new_notebook(cells=nb.cells or [nbformat.v4.new_code_cell('#')]);nbformat.validator.normalize(nb)
      nbformat.write(nb,(p/f'{n.stem}.ipynb').open('w',encoding='utf-8'))
      with (p/'config.yml').open('w', encoding='utf-8') as f: yaml.dump({'include-in-header':[{'text':r'\usepackage{fvextra}\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\{\}}'}],'include-before-body':[{'text':r'\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}'}]},f)
      !quarto render {p}/{n.stem}.ipynb --metadata-file={p}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet
      google.colab.files.download(str(p/f'{n.stem}.pdf'));s.value=f'‚úÖ Downloaded: {n.stem}.pdf'
    except Exception as e:s.value=f'‚ùå {str(e)}'
    finally:b.disabled=False
  if not pathlib.Path('/usr/local/bin/quarto').exists():
    !wget -q 'https://quarto.org/download/latest/quarto-linux-amd64.deb' && dpkg -i quarto-linux-amd64.deb>/dev/null && quarto install tinytex --update-path --quiet && rm quarto-linux-amd64.deb
  b=ipywidgets.widgets.Button(description='‚¨áÔ∏è Download');s=ipywidgets.widgets.Label();b.on_click(lambda b:convert(b));IPython.display.display(ipywidgets.widgets.HBox([b,s]))
colab2pdf() # | Colab2PDF v1.6 | https://github.com/drengskapur/colab2pdf | GPL-3.0-or-later |